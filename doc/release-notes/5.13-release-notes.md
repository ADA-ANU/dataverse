# Dataverse Software 5.13

This release brings new features, enhancements, and bug fixes to the Dataverse Software. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.

## Release Highlights


## Mechanism added for stopping a harvest in progress

It is now possible for an admin to stop a long-running harvesting job. See [Harvesting Clients](https://guides.dataverse.org/en/latest/admin/harvestclients.html) guide for more information. 

## License sorting

Licenses as shown in the dropdown in UI can be now sorted by the superusers. See [Configuring Licenses](https://guides.dataverse.org/en/5.10/installation/config.html#configuring-licenses) section of the Installation Guide for reference.


## Schema.org changes

The Schema.org metadata export and the schema.org metadata embedded in dataset pages has been updated to improve compliance with Schema.org's schema and Google's recommendations.

Backward compatibility:
 - descriptions are now joined and truncated to less than 5K characters.
 - the "citation"/"text" key has been replaced by a "citation"/"name" key.
 - file entries now have the mimetype reported as 'encodingFormat' rather than 'fileFormat' to better conform with the Schema.org specification for DataDownload entries. Download URLs are now sent for all files unless the dataverse.files.hide-schema-dot-org-download-urls setting is set to true.
 - author/creators now have an @type of Person or Organization and any affiliation (affiliation for Person, parentOrganization for Organization) is now an object of @type Organization

## Improved Security for External Tools

This release adds support for configuring external tools to use signed URLs to access the Dataverse API. This eliminates the need for tools to have access to the user's apiToken in order to access draft or restricted datasets and datafiles. Signed URLS can be transferred via POST or via a callback when triggering a tool via GET.

## Experimental CodeMeta Schema Support

With this release, we are adding "experimental" (see note below) support for research software metadata deposits.

By adding a metadata block for [CodeMeta](https://codemeta.github.io), we take another step extending the Dataverse
scope being a research data repository towards first class support of diverse F.A.I.R. objects, currently focusing
on research software and computational workflows.

There is more work underway to make Dataverse installations around the world "research software ready". We hope
for feedback from installations on the new metadata block to optimize and lift it from the experimental stage.

**Note:** like the metadata block for computational workflows before, this schema is flagged as "experimental".
"Experimental" means it's brand new, opt-in, and might need future tweaking based on experience of usage in the field.
These blocks are listed here: https://guides.dataverse.org/en/latest/user/appendix.html#experimental-metadata

### Default Values for Database Connections Fixed

Introduced in Dataverse release 5.3 a regression might have hit you:
the announced default values for the database connection never actually worked.

With the update to Payara 5.2022.3 it was possible to introduce working
defaults. The documentation has been changed accordingly.

Together with this change, you can now enable advanced connection pool
configurations useful for debugging and monitoring. Of particular interest may be `sslmode=require`. See the docs for details.

### Metadata field Production Location now repeatable, facetable, and enabled for Advanced Search

This enhancement allows depositors to define multiple instances of the metadata field Production Location in the Citation Metadata block, users to filter search results using the filter facets, and using the field in the Advanced Search option.

### CSRT PID Types Added to Related Publication ID Type field

A persistent identifier, [CSRT](https://www.cstr.cn/search/specification/), is added to the Related Publication field's ID Type child field. For datasets published with CSRT IDs, Dataverse will also include them in the datasets' Schema.org metadata exports.

### API endpoint related to metadata blocks has been updated

The API endpoint `/api/metadatablocks/{block_id}` has been extended to include the following fields:

- `controlledVocabularyValues` - All possible values for fields with a controlled vocabulary. For example, the values "Agricultural Sciences", "Arts and Humanities", etc. for the "Subject" field.
- `isControlledVocabulary`:  Whether or not this field has a controlled vocabulary.
- `multiple`: Whether or not the field supports multiple values.

### Support for .eln files has been added.

See file format specification: https://github.com/TheELNConsortium/TheELNFileFormat

The .eln file format is used by Electronic Laboratory Notebooks as an exchange format for experimental protocols, results, sample descriptions, etc...

In order for existing .eln files to show as zip files (instead of unknown), you must run the file re-detection API on them and reindex them into Solr.


### Support for cleaning up files in datasets' storage

Experimental feature: the leftover files stored in the Dataset storage location that are not in the file list of that Dataset, but are named following the Dataverse technical convetion for dataset files, can be removed with the new native API call [Cleanup storage of a Dataset](https://guides.dataverse.org/en/latest/api/native-api.html#cleanup-storage-api).


## Major Use Cases and Infrastructure Enhancements

Changes and fixes in this release include:

- Administrators can configure a different place for storage of generated temporary files. (PR #8983)
- Binder has been added to the list of external tools that can be added to a Dataverse installation. From the dataset page, you can launch Binder, which spins up a computational environment in which you can explore the code and data in the dataset, or write new code, such as a Jupyter notebook. (PR #9341)
- Support for indexing the "Geographic Bounding Box" fields ("West Longitude", "East Longitude", "North Latitude", and "South Latitude") from the Geospatial metadata block has been added.
- Geospatial search is supported but only via API using two new parameters: `geo_point` and `geo_radius`.
- To improve performance, Dataverse estimates download counts. This release includes an update that makes the estimate more accurate.
- Direct upload and out-of-band uploads can now be used to replace multiple files with one API call (complementing the prior ability to add multiple new files)
- Dataverse can now support upload of an entire folder tree of files and retain the relative paths of files as directory path metadata for the uploaded files, if the installation is configured with S3 direct upload.
- NetCDF and HDF5 files are now detected based on their content rather than just their file extension.
- Both "classic" NetCDF 3 files and more modern NetCDF 4 files are detected based on content.
- Detection for HDF4 files is only done through the file extension ".hdf", as before.
- For NetCDF and HDF5 files, an attempt will be made to extract metadata in NcML (XML) format and save it as an auxiliary file.
- An "extractNcml" API endpoint has been added, especially for installations with existing NetCDF and HDF5 files. After upgrading, they can iterate through these files and try to extract an NcML file.
- Data contained in a dataset may have been produced at multiple places. Making the field Production Location repeatable will make it possible to reflect this fact in the dataset metadata. Making the field facetable and enabled for Advanced Search will allow us to customize Dataverse collections more appropriately. (Issue #9253, PR #9254)


## New JVM Options
The following JVM options are now available:

-dataverse.personOrOrg.assumeCommaInPersonName, default is false
-dataverse.files.uploads

## Notes for Dataverse Installation Administrators


## Notes for Developers and Integrators

See the "Backward Incompatibilities" section below.

## Backward Incompatibilities

License files are now required to contain the new "sortOrder" column. When attempting to create a new license without this field, an error would be returned. See [Configuring Licenses](https://guides.dataverse.org/en/5.10/installation/config.html#configuring-licenses) section of the Installation Guide for reference.

## Installation

If this is a new installation, please see our [Installation Guide](https://guides.dataverse.org/en/5.13/installation/). Please also contact us to get added to the [Dataverse Project Map](https://guides.dataverse.org/en/5.10/installation/config.html#putting-your-dataverse-installation-on-the-map-at-dataverse-org) if you have not done so already.

## Upgrade Instructions

0\. These instructions assume that you've already successfully upgraded from Dataverse Software 4.x to Dataverse Software 5 following the instructions in the [Dataverse Software 5 Release Notes](https://github.com/IQSS/dataverse/releases/tag/v5.0). After upgrading from the 4.x series to 5.0, you should progress through the other 5.x releases before attempting the upgrade to 5.13.

If you are running Payara as a non-root user (and you should be!), **remember not to execute the commands below as root**. Use `sudo` to change to that user first. For example, `sudo -i -u dataverse` if `dataverse` is your dedicated application user.

In the following commands we assume that Payara 5 is installed in `/usr/local/payara5`. If not, adjust as needed.

`export PAYARA=/usr/local/payara5`

(or `setenv PAYARA /usr/local/payara5` if you are using a `csh`-like shell)

1\. Undeploy the previous version.

- `$PAYARA/bin/asadmin list-applications`
- `$PAYARA/bin/asadmin undeploy dataverse<-version>`

2\. Stop Payara and remove the generated directory

- `service payara stop`
- `rm -rf $PAYARA/glassfish/domains/domain1/generated`

3\. Start Payara

- `service payara start`

4\. Deploy this version.

- `$PAYARA/bin/asadmin deploy dataverse-5.13.war`

5\. Restart Payara

- `service payara stop`
- `service payara start`

6\. Reload citation metadata block

   `wget https://github.com/IQSS/dataverse/releases/download/v5.11/citation.tsv`
   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @citation.tsv -H "Content-type: text/tab-separated-values"`
- Add the updated citation.properties file to the appropriate directory

7\. Replace Solr schema.xml to allow multiple production locations and support for geospatial indexing to be used. See specific instructions below for those installations without custom metadata blocks (1a) and those with  custom metadata blocks  (1b).

7a\.

For installations without Custom Metadata Blocks:

-stop solr instance (usually service solr stop, depending on solr installation/OS, see the [Installation Guide](https://guides.dataverse.org/en/5.13/installation/prerequisites.html#solr-init-script)

-replace schema.xml

cp /tmp/dvinstall/schema.xml /usr/local/solr/solr-8.11.1/server/solr/collection1/conf

-start solr instance (usually service solr start, depending on solr/OS)


7b\. 

For installations with Custom Metadata Blocks:

-stop solr instance (usually service solr stop, depending on solr installation/OS, see the [Installation Guide](https://guides.dataverse.org/en/5.13/installation/prerequisites.html#solr-init-script)

- edit the following line to your schema.xml (to indicate that productionPlace is now multiValued='true"):

    `<field name="productionPlace" type="string" stored="true" indexed="true" multiValued="true"/>`

- add the following lines to your schema.xml to add support for geospatial indexing:

    `<!-- Dataverse geospatial search -->`
    `<!-- https://solr.apache.org/guide/8_11/spatial-search.html#rpt -->`
    `<field name="geolocation" type="location_rpt" multiValued="true" stored="true" indexed="true"/>`
    `<!-- https://solr.apache.org/guide/8_11/spatial-search.html#bboxfield -->`
    `<field name="boundingBox" type="bbox" multiValued="true" stored="true" indexed="true"/>`
    `<!-- Dataverse - per GeoBlacklight, adding field type for bboxField that enables, among other things, overlap ratio calculations -->`
    `<fieldType name="bbox" class="solr.BBoxField"
           geo="true" distanceUnits="kilometers" numberType="pdouble" />`

- restart solr instance (usually service solr start, depending on solr/OS)

# Optional Upgrade Steps: remove Workflow Schema fields from Solr index

In Dataverse 5.12 we added a new experimental metadata schema block for workflow deposition.
We included the fields within the standard Solr schema we provide. With this version, we
removed it from the schema. If you are deploying the block to your installation, make sure to
update your index.

If you already added these fields, you can delete them from your index when not using the schema.
Make sure to [reindex after changing the schema](https://guides.dataverse.org/en/latest/admin/solr-search-index.html?highlight=reindex#reindex-in-place.

Remember: depending on the size of your installation, reindexing may take serious time to complete.
You should do this in off-hours.

# Optional Upgrade Steps: reindex linked dataverse collections

Datasets that are part of linked dataverse collections will now be displayed in 
their linking dataverse collections. In order to fix the display of collections 
that have already been linked you must re-index the linked collections. This 
query will provide a list of commands to re-index the effected collections:

select 'curl http://localhost:8080/api/admin/index/dataverses/' 
|| tmp.dvid  from (select distinct  dataverse_id as dvid  
from dataverselinkingdataverse)  as tmp

The result of the query will be a list of re-index commands such as:

curl http://localhost:8080/api/admin/index/dataverses/633

where '633' is the id of the linked collection.