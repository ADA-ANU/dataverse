# Dataverse 5.0

This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.

Please note that this is a major release and these are long release notes. We offer no apologies. :)

## Release Highlights

### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout

The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.

This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.

### Payara 5

A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.

Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.

### Primefaces 8

Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.

### Download Dataset

Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.

#### Download All Option on the Dataset Page

In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the "Access Dataset" button.

#### Download All API

In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:

- Find all the database id of the files.
- Download all the files, using those ids (comma-separated).

Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the "download metadata" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).

### A Multi-File, Zipped Download Optimization

In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.

Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.

The components of the standalone "zipper tool" can also be downloaded
here:

XXXXX

(my plan is to build the executable and to add it to the v5
release files on github: - L.A.)
https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip.

### Updated File Handling

Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:

- Files with the same checksum can be included in a dataset, even if the files are in the same directory.
- Files with the same filename can be included in a dataset as long as the files are in different directories.
- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding "-1" or "-2" as applicable. This change will be visible in the list of files being uploaded.
- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.
- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.
- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.
- Files without extensions can now be uploaded through the UI.

### Pre-Publish DOI Reservation with DataCite

Dataverse Installations using DataCite will have DOIs reserved as "Draft" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.

## Major Use Cases

Newly-supported use cases in this release include:

- Buttons 
- Responsive
- Download All (Issue #6564, PR #6262)
- DOI reservation for datasets
- Files without extensions
- Files with same name in diff directories
- Files with same MD5 in Dataset
- Email Domain Based groups (Issue #6936, PR #6974)
- Date Facet Changes
- Link Harvested Datasets 5886/6935
- Destroy Dataset with Mapped Datafile (PR #6860)
- Publishing Improvements
- Fewer locks #6918/#7118

## Notes for Dataverse Installation Administrators

### Glassfish to Payara

See the detailed upgrade instructions below.

### Datafiles Validation when Publishing Datasets

When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.

If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the "Troubleshooting" section of the Guide on how to fix such problems.

For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.

Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).

### The setting :PIDAsynchRegFileCount is deprecated as of v5.0.

It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.

### Location Changes for Related Projects

The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:

<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>
<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>

### Harvesting Improvements

Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.

### New JVM Options and Database Settings

Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/latest/installation/config.rst).

#### New JVM Options

- If you are using DataCite as your DOI provider you must add a new JVM option called "doi.baseurlstringnext" with a value of "https://api.datacite.org" for production environments and "https://api.test.datacite.org" for test environments. More information about this JVM option can be found in the Installation Guide.
- dataverse.useripaddresssourceheader
- FileValidationOnPublishEnabled
- New JVM Option for Zip Download Timeout
- http.request-timeout-seconds To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes). Should you wish to shorten or lengthen this window, issue for example:

``./asadmin set server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds=3600``

and restart Payara to apply your change.

#### New Database Settings

- CustomZipDownloadServiceUrl If defined, this is the URL of the zipping service outside the main Application Service where zip downloads should be directed (instead of /api/access/datafiles/)
- :ShibAttributeCharacterSetConversionEnabled Shib Character Conversion By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting  to false.
- :ChronologicalDateFacets setting. Default to true.Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false
- :NavbarGuidesUrl
- :PIDAsynchRegFileCount

### Custom Analytics Code Changes

You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.

We have updated the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/latest/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).

### Tracking users' IP addresses behind an address-masking proxy

It is now possible to collect real user IP addresses in MDC logs and/or set up an IP group on a system running behind a proxy/load balancer that hides the addresses of incoming requests. See "Recording User IP Addresses" in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/latest/installation/config.rst).

### Reload Astrophysics Metadata Block (if used)

Tooltips have been updated for the Astrophysics Metadata Block. If you'd like these updated Tooltips to be displayed to users of your installation, you should update the Astrophysics Metadata Block:

`curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H "Content-type: text/tab-separated-values"` 

We've included this in the step-by-step instructions below.

### Run ReExportall

We made changes to the JSON Export in this release. If you'd like these changes to reflected in your JSON exports, you should run ReExportall as part of the upgrade process following the steps in [Admin Guide](http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api)

We've included this in the step-by-step instructions below.

## Notes for Tool Developers and Integrators

## Complete List of Changes

For the complete list of code changes in this release, see the [5.0 Milestone](https://github.com/IQSS/dataverse/milestone/89?closed=1) in Github.

For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.

## Installation

If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.0/installation/)

## Upgrade

Upgrade Dataverse from Glassfish 4.1 to Payara 5
================================================

The instruction below describes the upgrade procedure based on moving an existing glassfish4 domain directory under Payara. We recommend this method, instead of setting up a brand-new Payara domain using the installer because it appears to be the easiest way to recreate your current configuration and preserve all your data. 

Download Payara, v5.2020.2 as of this writing:

	# curl -L -O https://github.com/payara/Payara/releases/download/payara-server-5.2020.2/payara-5.2020.2.zip
	# sha256sum payara-5.2020.2.zip 
	  1f5f7ea30901b1b4c7bcdfa5591881a700c9b7e2022ae3894192ba97eb83cc3e

Unzip it somewhere (/usr/local is a safe bet)

	# sudo unzip payara-5.2020.2.zip -d /usr/local/

Copy the Postgres driver to /usr/local/payara5/glassfish/lib

	# sudo cp /usr/local/glassfish4/glassfish/lib/postgresql-42.2.9.jar /usr/local/payara5/glassfish/lib/

Move payara5/glassfish/domains/domain1 out of the way

	# sudo mv /usr/local/payara5/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/domain1.orig

Undeploy the Dataverse web application (if deployed; version 4.20 is assumed in the example below)

	# sudo /usr/local/glassfish4/bin/asadmin list-applications
	# sudo /usr/local/glassfish4/bin/asadmin undeploy dataverse-4.20

Stop Glassfish; copy domain1 to Payara

	# sudo /usr/local/glassfish4/bin/asadmin stop-domain
	# sudo cp -ar /usr/local/glassfish4/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/

Remove the Glassfish cache directories

	# sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/generated/
	# sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/osgi-cache/

In domain.xml:
=============

Replace the -XX:PermSize and -XX:MaxPermSize JVM options with -XX:MetaspaceSize and -XX:MaxMetaspaceSize.

        <jvm-options>-XX:MetaspaceSize=256m</jvm-options>
        <jvm-options>-XX:MaxMetaspaceSize=512m</jvm-options>

Set both Xmx and Xms at startup to avoid runtime re-allocation. Your Xmx value should likely be higher: 

	<jvm-options>-Xmx2048m</jvm-options>
	<jvm-options>-Xms2048m</jvm-options>

Add the below JVM options beneath the -Ddataverse settings:  

	<jvm-options>-Dfish.payara.classloading.delegate=false</jvm-options>
	<jvm-options>-XX:+UseG1GC</jvm-options>
	<jvm-options>-XX:+UseStringDeduplication</jvm-options>
	<jvm-options>-XX:+DisableExplicitGC</jvm-options>

Change any full pathnames /usr/local/glassfish4/... to /usr/local/payara5/... or whatever it is in your case. (Specifically check the -Ddataverse.files.directory and -Ddataverse.files.file.directory JVM options)

In domain1/config/jhove.conf, change the hard-coded /usr/local/glassfish4 path, as above.

(Optional): If you renamed your service account from glassfish to payara or appserver, update the ownership permissions. The Installation Guide recommends a service account of `dataverse`:

	# sudo chown -R dataverse /usr/local/payara5/glassfish/domains/domain1
	# sudo chown -R dataverse /usr/local/payara5/glassfish/lib
	
You will also need to check that the service account has write permission on the files directory, if they are located outside the old Glassfish domain. And/or make sure the service account has the correct AWS credentials, if you are using S3 for storage. 

Finally, start Payara:

	# sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain

Deploy the Dataverse 5 warfile:

	# sudo -u dataverse /usr/local/payara5/bin/asadmin deploy /path/to/dataverse-5.0.war

Then restart Payara:

	# sudo -u dataverse /usr/local/payara5/bin/asadmin stop-domain
	# sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain










1. Undeploy the previous version.

   - &lt;glassfish install path&gt;/glassfish4/bin/asadmin list-applications
   - &lt;glassfish install path&gt;/glassfish4/bin/asadmin undeploy dataverse

2. Stop glassfish and remove the generated directory, start.

   - service glassfish stop
   - remove the generated directory: rm -rf &lt;glassfish install path&gt;glassfish4/glassfish/domains/domain1/generated
   - service glassfish start

1. Deploy this version.

   - &lt;glassfish install path&gt;/glassfish4/bin/asadmin deploy &lt;path&gt;dataverse-4.20.war

2. Restart Payara.

3. Update Astrophysics Metadata Block (if used)

   - `wget https://github.com/IQSS/dataverse/releases/download/5.0/astrophysics.tsv`
   - `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H "Content-type: text/tab-separated-values"`

4. (Recommended) Run ReExportall to update JSON Exports  

   <http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api>
