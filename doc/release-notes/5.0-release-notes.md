# Dataverse 5.0

This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.

## Release Highlights

### Updated Continued Dataset and File Redesign - Dataset and File Button Redesign, Responsive Layout

The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability in data access and explore options, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive.

This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements.

### Payara 5

A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.

Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.

### Primefaces 8

Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.

### Zip Optimization and Download All Dataset

- reducing manifest occurrences
- system stability

## Major Use Cases

Newly-supported use cases in this release include:

- Buttons
- Responsive
- Download All (Issue #6564, PR #6262)
- DOI reservation for datasets
- Files without extensions
- Files with same name in diff directories
- Files with same MD5 in Dataset
- Email Domain Based groups (Issue #6936, PR #6974)
- Date Facet Changes
- Link Harvested Datasets
- Destroy Dataset with Mapped Datafile (PR #6860)
- Publishing Improvements

## Notes for Dataverse Installation Administrators

### Payara Steps

To be added in #6961

### Datafiles Validation when Publishing Datasets

When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.

If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the "Troubleshooting" section of the Guide on how to fix such problems.

For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.

Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).

### Location Changes for Related Projects

The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:

<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>
<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>

### Harvesting Improvements

Note Jing Ma's changes - #7057, #7062, #7053, #7004, #7010, #7024

### New JVM Options and Database Settings

#### New JVM Options for DOI Reservation

If you are using DataCite as your DOI provider you must add a new JVM option called "doi.baseurlstringnext" with a value of "https://api.datacite.org" for production environments and "https://api.test.datacite.org" for test environments. More information about this JVM option can be found in the Installation Guide.

#### New JVM Option for Verifying Files at Publish

:FileValidationOnPublishEnabled

#### New Database Setting for Shibboleth Character Conversion

By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting :ShibAttributeCharacterSetConversionEnabled to false.

For further documentation on this issue, see the [Installation Guide](http://guides.dataverse.org/en/latest/installation/config.rst)

#### New Database Settings for Search Facet Sorting

The new :ChronologicalDateFacets setting. Default to true.

Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false

### Custom Analytics Code Changes

You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.

We have updated the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/latest/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).

### Tracking users' IP addresses behind an address-masking proxy

It is now possible to collect real user IP addresses in MDC logs and/or set up an IP group on a system running behind a proxy/load balancer that hides the addresses of incoming requests. See "Recording User IP Addresses" in the Configuration section of the Installation Guide.

### Reload Astrophysics Metadata Block (if used)

`curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H "Content-type: text/tab-separated-values"`  

### Run ReExportall

We made changes to the JSON Export in this release to the stylesheet that generates the DDI HTML Codebook metadata export to remove a hard-coded reference to ICPSR. If you'd like these changes to reflected in your JSON exports, you should run ReExportall as part of the upgrade process. We've included this in the step-by-step instructions below.

## Notes for Tool Developers and Integrators

## Complete List of Changes

For the complete list of code changes in this release, see the [5.0 Milestone](https://github.com/IQSS/dataverse/milestone/89?closed=1) in Github.

For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.

## Installation

If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.0/installation/)

## Upgrade

1. Undeploy the previous version.

   - &lt;glassfish install path&gt;/glassfish4/bin/asadmin list-applications
   - &lt;glassfish install path&gt;/glassfish4/bin/asadmin undeploy dataverse

2. Stop glassfish and remove the generated directory, start.

   - service glassfish stop
   - remove the generated directory: rm -rf &lt;glassfish install path&gt;glassfish4/glassfish/domains/domain1/generated
   - service glassfish start

3. Install and configure Solr v7.7.2

    See <http://guides.dataverse.org/en/4.20/installation/prerequisites.html#installing-solr>

4. Deploy this version.

   - &lt;glassfish install path&gt;/glassfish4/bin/asadmin deploy &lt;path&gt;dataverse-4.20.war

5. Restart Payara.

6. Update Citation Metadata Block

   - `wget https://github.com/IQSS/dataverse/releases/download/4.20/citation.tsv`
   - `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @citation.tsv -H "Content-type: text/tab-separated-values"`

7. Kick off full reindex

    <http://guides.dataverse.org/en/4.20/admin/solr-search-index.html>

8. (Recommended) Run ReExportall to update JSON Exports  

   <http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api>
